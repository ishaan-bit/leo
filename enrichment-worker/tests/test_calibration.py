"""
Tests for confidence calibration
"""

import pytest
import numpy as np
from src.enrich.calibration import (
    ConfidenceCalibrator,
    CalibrationMetrics,
    compare_calibration_methods
)


def test_perfect_calibration():
    """Test ECE with perfectly calibrated confidences"""
    # Perfect calibration: confidence = accuracy
    confidences = np.array([0.1, 0.3, 0.5, 0.7, 0.9] * 20)
    correctness = np.random.rand(100) < confidences  # Stochastic but matches conf on average
    
    calibrator = ConfidenceCalibrator(method="none")
    metrics = calibrator.calculate_ece(confidences, correctness)
    
    # With enough samples, ECE should be low (not zero due to sampling)
    assert metrics.ece < 0.15  # Reasonable threshold for stochastic test
    assert metrics.num_bins == 10


def test_temperature_scaling():
    """Test temperature scaling calibration"""
    np.random.seed(42)
    
    # Overconfident predictions
    confidences = np.random.beta(8, 2, 100)  # Skewed high
    correctness = (np.random.rand(100) < 0.6).astype(int)  # 60% accuracy
    
    calibrator = ConfidenceCalibrator(method="temperature")
    calibrator.fit(confidences, correctness)
    
    # Temperature should be > 1 (softening overconfidence)
    assert calibrator.temperature > 1.0
    
    # Calibrated confidences should be lower
    calibrated = calibrator.calibrate(confidences)
    assert calibrated.mean() < confidences.mean()


def test_platt_scaling():
    """Test Platt scaling calibration"""
    np.random.seed(42)
    
    confidences = np.random.rand(100)
    correctness = (np.random.rand(100) < 0.5).astype(int)
    
    calibrator = ConfidenceCalibrator(method="platt")
    calibrator.fit(confidences, correctness)
    
    assert calibrator.is_fitted
    assert calibrator.platt_model is not None
    
    # Can calibrate
    calibrated = calibrator.calibrate(confidences)
    assert len(calibrated) == len(confidences)
    assert np.all((calibrated >= 0) & (calibrated <= 1))


def test_isotonic_regression():
    """Test isotonic regression calibration"""
    np.random.seed(42)
    
    confidences = np.random.rand(100)
    correctness = (np.random.rand(100) < 0.5).astype(int)
    
    calibrator = ConfidenceCalibrator(method="isotonic")
    calibrator.fit(confidences, correctness)
    
    assert calibrator.is_fitted
    assert calibrator.isotonic_model is not None
    
    # Can calibrate
    calibrated = calibrator.calibrate(confidences)
    assert len(calibrated) == len(confidences)
    assert np.all((calibrated >= 0) & (calibrated <= 1))


def test_ece_calculation():
    """Test ECE calculation with known values"""
    # Simple case: 10 predictions
    confidences = np.array([0.9, 0.8, 0.7, 0.6, 0.5, 0.5, 0.4, 0.3, 0.2, 0.1])
    correctness = np.array([1, 1, 1, 0, 1, 0, 0, 0, 0, 0])
    
    calibrator = ConfidenceCalibrator(method="none", n_bins=5)
    metrics = calibrator.calculate_ece(confidences, correctness, n_bins=5)
    
    # Should calculate some ECE
    assert 0 <= metrics.ece <= 1
    assert metrics.num_bins == 5
    assert len(metrics.bin_accuracies) == 5
    assert len(metrics.bin_confidences) == 5
    assert len(metrics.bin_counts) == 5


def test_calibration_improves_ece():
    """Test that calibration reduces ECE"""
    np.random.seed(42)
    
    # Generate miscalibrated data
    confidences = np.random.beta(8, 2, 200)  # Overconfident
    true_accuracy = 0.6
    correctness = (np.random.rand(200) < true_accuracy).astype(int)
    
    # Split train/test
    train_conf = confidences[:150]
    train_correct = correctness[:150]
    test_conf = confidences[150:]
    test_correct = correctness[150:]
    
    # Calculate uncalibrated ECE
    calibrator_none = ConfidenceCalibrator(method="none")
    uncalibrated_metrics = calibrator_none.calculate_ece(test_conf, test_correct)
    
    # Calibrate with temperature scaling
    calibrator_temp = ConfidenceCalibrator(method="temperature")
    calibrator_temp.fit(train_conf, train_correct)
    calibrated_conf = calibrator_temp.calibrate(test_conf)
    calibrated_metrics = calibrator_temp.calculate_ece(calibrated_conf, test_correct)
    
    # Calibration should reduce ECE (usually, but not guaranteed with small sample)
    # Just check it doesn't increase it dramatically
    assert calibrated_metrics.ece <= uncalibrated_metrics.ece * 1.5


def test_compare_methods():
    """Test comparison of calibration methods"""
    np.random.seed(42)
    
    confidences = np.random.rand(100)
    correctness = (np.random.rand(100) < 0.5).astype(int)
    
    results = compare_calibration_methods(
        confidences, correctness, train_size=0.8, verbose=False
    )
    
    # Should return results for all methods
    assert "none" in results
    assert "temperature" in results
    assert "platt" in results
    assert "isotonic" in results
    
    # All should have valid ECE
    for method, metrics in results.items():
        assert 0 <= metrics.ece <= 1
        assert metrics.num_bins == 10


def test_calibration_metrics_structure():
    """Test CalibrationMetrics dataclass"""
    metrics = CalibrationMetrics(
        ece=0.05,
        mce=0.12,
        ace=0.06,
        num_bins=10,
        bin_accuracies=[0.1, 0.2, 0.3],
        bin_confidences=[0.15, 0.25, 0.28],
        bin_counts=[10, 15, 12]
    )
    
    assert metrics.ece == 0.05
    assert metrics.mce == 0.12
    assert metrics.ace == 0.06
    assert len(metrics.bin_accuracies) == 3


def test_temperature_scaling_bounds():
    """Test temperature scaling with edge cases"""
    calibrator = ConfidenceCalibrator(method="temperature")
    
    # Very confident
    confidences = np.array([0.99, 0.98, 0.97])
    temperature = 2.0  # Soften
    calibrated = calibrator._apply_temperature_scaling(confidences, temperature)
    
    # Should reduce confidence
    assert np.all(calibrated < confidences)
    assert np.all((calibrated >= 0) & (calibrated <= 1))
    
    # Very uncertain
    confidences = np.array([0.51, 0.52, 0.53])
    temperature = 0.5  # Sharpen
    calibrated = calibrator._apply_temperature_scaling(confidences, temperature)
    
    # Should increase distance from 0.5
    assert np.all(np.abs(calibrated - 0.5) >= np.abs(confidences - 0.5) - 0.01)


def test_ece_target_compliance():
    """Test that well-calibrated data meets ECE â‰¤ 0.08 target"""
    np.random.seed(42)
    
    # Generate well-calibrated data
    confidences = np.random.rand(500)
    # Accuracy matches confidence (with some noise)
    correctness = (np.random.rand(500) < confidences).astype(int)
    
    calibrator = ConfidenceCalibrator(method="none")
    metrics = calibrator.calculate_ece(confidences, correctness)
    
    # With enough samples and true calibration, ECE should be low
    # Note: This is stochastic, so we allow some margin
    assert metrics.ece < 0.15  # Reasonable for stochastic test


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
